{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5\n",
    "\n",
    "Aditya Kharosekar amk3587\n",
    "Rahul Jain rj8656"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 4 Problem 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weekly = pd.read_csv(\"Weekly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-3.484</td>\n",
       "      <td>0.154976</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1990</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.148574</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1990</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>0.159837</td>\n",
       "      <td>3.514</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1990</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>0.712</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.712</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.153728</td>\n",
       "      <td>1.178</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today  \\\n",
       "0           1  1990  0.816  1.572 -3.936 -0.229 -3.484  0.154976 -0.270   \n",
       "1           2  1990 -0.270  0.816  1.572 -3.936 -0.229  0.148574 -2.576   \n",
       "2           3  1990 -2.576 -0.270  0.816  1.572 -3.936  0.159837  3.514   \n",
       "3           4  1990  3.514 -2.576 -0.270  0.816  1.572  0.161630  0.712   \n",
       "4           5  1990  0.712  3.514 -2.576 -0.270  0.816  0.153728  1.178   \n",
       "\n",
       "  Direction  \n",
       "0      Down  \n",
       "1      Down  \n",
       "2        Up  \n",
       "3        Up  \n",
       "4        Up  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1089, 10)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NEED TO DO NUMERICAL/GRAPHICAL SUMMARIES **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-3.484</td>\n",
       "      <td>0.154976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.148574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>0.159837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>0.161630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.712</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.153728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Lag1   Lag2   Lag3   Lag4   Lag5    Volume\n",
       "0  0.816  1.572 -3.936 -0.229 -3.484  0.154976\n",
       "1 -0.270  0.816  1.572 -3.936 -0.229  0.148574\n",
       "2 -2.576 -0.270  0.816  1.572 -3.936  0.159837\n",
       "3  3.514 -2.576 -0.270  0.816  1.572  0.161630\n",
       "4  0.712  3.514 -2.576 -0.270  0.816  0.153728"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg = LogisticRegression()\n",
    "testing = weekly.loc[:,\"Lag1\":\"Volume\"]\n",
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.fit(testing, weekly[\"Direction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.150585</td>\n",
       "      <td>0.151079</td>\n",
       "      <td>0.147205</td>\n",
       "      <td>0.145818</td>\n",
       "      <td>0.139893</td>\n",
       "      <td>1.574618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.357013</td>\n",
       "      <td>2.357254</td>\n",
       "      <td>2.360502</td>\n",
       "      <td>2.360279</td>\n",
       "      <td>2.361285</td>\n",
       "      <td>1.686636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>0.087465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.154000</td>\n",
       "      <td>-1.154000</td>\n",
       "      <td>-1.158000</td>\n",
       "      <td>-1.158000</td>\n",
       "      <td>-1.166000</td>\n",
       "      <td>0.332022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>1.002680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.405000</td>\n",
       "      <td>1.409000</td>\n",
       "      <td>1.409000</td>\n",
       "      <td>1.409000</td>\n",
       "      <td>1.405000</td>\n",
       "      <td>2.053727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>9.328214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Lag1         Lag2         Lag3         Lag4         Lag5  \\\n",
       "count  1089.000000  1089.000000  1089.000000  1089.000000  1089.000000   \n",
       "mean      0.150585     0.151079     0.147205     0.145818     0.139893   \n",
       "std       2.357013     2.357254     2.360502     2.360279     2.361285   \n",
       "min     -18.195000   -18.195000   -18.195000   -18.195000   -18.195000   \n",
       "25%      -1.154000    -1.154000    -1.158000    -1.158000    -1.166000   \n",
       "50%       0.241000     0.241000     0.241000     0.238000     0.234000   \n",
       "75%       1.405000     1.409000     1.409000     1.409000     1.405000   \n",
       "max      12.026000    12.026000    12.026000    12.026000    12.026000   \n",
       "\n",
       "            Volume  \n",
       "count  1089.000000  \n",
       "mean      1.574618  \n",
       "std       1.686636  \n",
       "min       0.087465  \n",
       "25%       0.332022  \n",
       "50%       1.002680  \n",
       "75%       2.053727  \n",
       "max       9.328214  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix shows the result of describe() on the matrix made up of the predictor variables. All 5 Lag variables are very similar to each other. The Volume variable, by its nature, has a different structure, but just based on the above matrix, it is hard to say whether any of these variables are by themselves statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1089,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Up', 'Up', 'Up', ..., 'Up', 'Up', 'Up'], dtype=object)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = logReg.predict(testing)\n",
    "print(results.shape)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 55 429]\n",
      " [ 47 558]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(weekly[\"Direction\"], results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out which row corresponds to \"Up\" and which row corresponds to \"Down\" - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "up_correct = 0\n",
    "up_predicted_as_down = 0\n",
    "for num in range(1089):\n",
    "    if (weekly[\"Direction\"][num]==\"Up\" and results[num]==\"Up\"):\n",
    "        up_correct+=1\n",
    "    if (weekly[\"Direction\"][num]==\"Up\" and results[num]==\"Down\"):\n",
    "        up_predicted_as_down +=1\n",
    "print(up_correct)\n",
    "print(up_predicted_as_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the confusion matrix - \n",
    "55 Downs and 558 Ups are predicted correctly. 47 Ups are predicted as downs and 429 Downs are predicted as ups.\n",
    "\n",
    "Overall fraction of correct predictions - (55+558)/ (47+429+55+558) = 0.563"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression tested on 1990-2008 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>981</td>\n",
       "      <td>2008</td>\n",
       "      <td>12.026</td>\n",
       "      <td>-8.389</td>\n",
       "      <td>-6.198</td>\n",
       "      <td>-3.898</td>\n",
       "      <td>10.491</td>\n",
       "      <td>5.841565</td>\n",
       "      <td>-2.251</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>982</td>\n",
       "      <td>2008</td>\n",
       "      <td>-2.251</td>\n",
       "      <td>12.026</td>\n",
       "      <td>-8.389</td>\n",
       "      <td>-6.198</td>\n",
       "      <td>-3.898</td>\n",
       "      <td>6.093950</td>\n",
       "      <td>0.418</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>983</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-2.251</td>\n",
       "      <td>12.026</td>\n",
       "      <td>-8.389</td>\n",
       "      <td>-6.198</td>\n",
       "      <td>5.932454</td>\n",
       "      <td>0.926</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>984</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-2.251</td>\n",
       "      <td>12.026</td>\n",
       "      <td>-8.389</td>\n",
       "      <td>5.855972</td>\n",
       "      <td>-1.698</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>985</td>\n",
       "      <td>2008</td>\n",
       "      <td>-1.698</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-2.251</td>\n",
       "      <td>12.026</td>\n",
       "      <td>3.087105</td>\n",
       "      <td>6.760</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Year    Lag1    Lag2    Lag3    Lag4    Lag5    Volume  \\\n",
       "980         981  2008  12.026  -8.389  -6.198  -3.898  10.491  5.841565   \n",
       "981         982  2008  -2.251  12.026  -8.389  -6.198  -3.898  6.093950   \n",
       "982         983  2008   0.418  -2.251  12.026  -8.389  -6.198  5.932454   \n",
       "983         984  2008   0.926   0.418  -2.251  12.026  -8.389  5.855972   \n",
       "984         985  2008  -1.698   0.926   0.418  -2.251  12.026  3.087105   \n",
       "\n",
       "     Today Direction  \n",
       "980 -2.251      Down  \n",
       "981  0.418        Up  \n",
       "982  0.926        Up  \n",
       "983 -1.698      Down  \n",
       "984  6.760        Up  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_2009 = weekly.loc[:984, :]\n",
    "pre_2009.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem a great way to separate the pre-2009 data from the rest but for now, this is how I will do it. I will change this when I figure out how to do this a better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>986</td>\n",
       "      <td>2009</td>\n",
       "      <td>6.760</td>\n",
       "      <td>-1.698</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-2.251</td>\n",
       "      <td>3.793110</td>\n",
       "      <td>-4.448</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>987</td>\n",
       "      <td>2009</td>\n",
       "      <td>-4.448</td>\n",
       "      <td>6.760</td>\n",
       "      <td>-1.698</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.418</td>\n",
       "      <td>5.043904</td>\n",
       "      <td>-4.518</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>988</td>\n",
       "      <td>2009</td>\n",
       "      <td>-4.518</td>\n",
       "      <td>-4.448</td>\n",
       "      <td>6.760</td>\n",
       "      <td>-1.698</td>\n",
       "      <td>0.926</td>\n",
       "      <td>5.948758</td>\n",
       "      <td>-2.137</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>989</td>\n",
       "      <td>2009</td>\n",
       "      <td>-2.137</td>\n",
       "      <td>-4.518</td>\n",
       "      <td>-4.448</td>\n",
       "      <td>6.760</td>\n",
       "      <td>-1.698</td>\n",
       "      <td>6.129763</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>990</td>\n",
       "      <td>2009</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-2.137</td>\n",
       "      <td>-4.518</td>\n",
       "      <td>-4.448</td>\n",
       "      <td>6.760</td>\n",
       "      <td>5.602004</td>\n",
       "      <td>5.173</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today  \\\n",
       "985         986  2009  6.760 -1.698  0.926  0.418 -2.251  3.793110 -4.448   \n",
       "986         987  2009 -4.448  6.760 -1.698  0.926  0.418  5.043904 -4.518   \n",
       "987         988  2009 -4.518 -4.448  6.760 -1.698  0.926  5.948758 -2.137   \n",
       "988         989  2009 -2.137 -4.518 -4.448  6.760 -1.698  6.129763 -0.730   \n",
       "989         990  2009 -0.730 -2.137 -4.518 -4.448  6.760  5.602004  5.173   \n",
       "\n",
       "    Direction  \n",
       "985      Down  \n",
       "986      Down  \n",
       "987      Down  \n",
       "988      Down  \n",
       "989        Up  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_2009 = weekly.loc[985:, :]\n",
    "post_2009.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985,)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag2_training  = pre_2009[\"Lag2\"]\n",
    "lag2_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:224: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return reshape(newshape, order=order)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag2_training = np.reshape(lag2_training, (985, 1)) # to get rid of a deprecation warning\n",
    "new_logistic_regr = LogisticRegression()\n",
    "new_logistic_regr.fit(lag2_training, pre_2009[\"Direction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104,)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag2_testing = post_2009[\"Lag2\"]\n",
    "lag2_testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9 34]\n",
      " [ 5 56]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:224: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return reshape(newshape, order=order)\n"
     ]
    }
   ],
   "source": [
    "lag2_testing = np.reshape(lag2_testing, (104, 1))\n",
    "new_results = new_logistic_regr.predict(lag2_testing)\n",
    "print(confusion_matrix(post_2009[\"Direction\"], new_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the same row-wise interpretation of the confusion matrix as before, the interpretation of the matrix is as follows - \n",
    "9 Downs and 56 Ups are predicted correctly. 5 Ups are predicted as Downs and 34 Downs are predicted as Ups.\n",
    "Overall fraction of correct predictions = (9+34)/(9+34+5+56) = 0.413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 1)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag2_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA()\n",
    "lda.fit(lag2_training, pre_2009[\"Direction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_results = lda.predict(lag2_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9 34]\n",
      " [ 5 56]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(post_2009[\"Direction\"], lda_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the exact same confusion matrix as the last logistic regression model. Let's see if both the models have the same predictions for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.85074208 -0.55704038] [-0.85097261 -0.55686856]\n",
      "[-0.82583009 -0.57602457] [-0.82621748 -0.57572292]\n",
      "[-0.79087438 -0.60412607] [-0.79147622 -0.60362703]\n",
      "[-0.719923   -0.66706965] [-0.72093726 -0.66610856]\n",
      "[-0.91688866 -0.5104272 ] [-0.91668814 -0.51056077]\n"
     ]
    }
   ],
   "source": [
    "x1 = new_logistic_regr.predict_log_proba(lag2_training)\n",
    "x2 = lda.predict_log_proba(lag2_training)\n",
    "for num in range(5): #printing out only the first few\n",
    "    print(x1[num], x2[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the log probabilities returned by both these models are not the same, the fact that both these models return the same confusion matrix is not an issue.\n",
    "\n",
    "Interpretration of the confusion matrix - \n",
    "9 Downs and 56 Ups are predicted correctly. 5 Ups are predicted as Downs and 34 Downs are predicted as Ups. Overall fraction of correct predictions = (9+34)/(9+34+5+56) = 0.413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
       "               store_covariances=False, tol=0.0001)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "qda = QDA()\n",
    "qda.fit(lag2_training, pre_2009[\"Direction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 43]\n",
      " [ 0 61]]\n"
     ]
    }
   ],
   "source": [
    "qda_results = qda.predict(lag2_testing)\n",
    "print(confusion_matrix(post_2009[\"Direction\"], qda_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "61 Ups are predicted correctly and 43 Downs are predicted as Ups. So it looks like this model is predicting everything as an Up. I am not sure if this is supposed to happen.\n",
    "\n",
    "Overall fraction of accurate predictions - 61/104 = 0.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "knn = KNN()\n",
    "knn.fit(lag2_training, pre_2009[\"Direction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16 27]\n",
      " [22 39]]\n"
     ]
    }
   ],
   "source": [
    "knn_results = knn.predict(lag2_testing)\n",
    "print(confusion_matrix(post_2009[\"Direction\"], knn_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 Downs are predicted correctly and 39 Ups are predicted correctly. 22 Ups are predicted as Downs and 27 Downs are predicted as Ups.\n",
    "\n",
    "Overall fraction of accurate predictions - (16+39)/(16+39+22+27) = 0.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QDA seems to provide the best results with an accuracy of 0.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "default = pd.read_csv(\"Default.csv\")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed() #setting random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.62507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.13470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.13895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.49394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.49588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 default student      balance       income\n",
       "0           1      No      No   729.526495  44361.62507\n",
       "1           2      No     Yes   817.180407  12106.13470\n",
       "2           3      No      No  1073.549164  31767.13895\n",
       "3           4      No      No   529.250605  35704.49394\n",
       "4           5      No      No   785.655883  38463.49588"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.62507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.13470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.13895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.49394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.49588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       balance       income\n",
       "0   729.526495  44361.62507\n",
       "1   817.180407  12106.13470\n",
       "2  1073.549164  31767.13895\n",
       "3   529.250605  35704.49394\n",
       "4   785.655883  38463.49588"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = default.loc[:,\"balance\":\"income\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    No\n",
       "1    No\n",
       "2    No\n",
       "3    No\n",
       "4    No\n",
       "Name: default, dtype: object"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = default[\"default\"]\n",
    "print(y.shape)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With default train/test split (75% train, 25% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regr = LogisticRegression()\n",
    "logistic_regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_results = logistic_regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = pd.Series(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, logistic_results)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error:  0.0356\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation error: \", 1-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now with 3 different data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, I will try a 50-50 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 2), (5000, 2))"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_regr = LogisticRegression()\n",
    "logistic_regr.fit(X_train, y_train)\n",
    "logistic_results = logistic_regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96579999999999999"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, logistic_results)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error:  0.0342\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation error: \", 1-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, I will try a 65% train - 35% test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6500, 2), (3500, 2))"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.35)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96799999999999997"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regr = LogisticRegression()\n",
    "logistic_regr.fit(X_train, y_train)\n",
    "logistic_results = logistic_regr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, logistic_results)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error:  0.032\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation error: \", 1-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally I will try a 35% train - 65% test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 2), (6500, 2))"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.65)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96769230769230774"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regr = LogisticRegression()\n",
    "logistic_regr.fit(X_train, y_train)\n",
    "logistic_results = logistic_regr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, logistic_results)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error:  0.0323076923077\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation error: \", 1-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Comment on results **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dummy variable for student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.62507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.13470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.13895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.49394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.49588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  student      balance       income\n",
       "0      No   729.526495  44361.62507\n",
       "1     Yes   817.180407  12106.13470\n",
       "2      No  1073.549164  31767.13895\n",
       "3      No   529.250605  35704.49394\n",
       "4      No   785.655883  38463.49588"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = default.loc[:,\"student\" :\"income\"]\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "      <th>student_No</th>\n",
       "      <th>student_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.62507</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.13470</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.13895</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.49394</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.49588</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       balance       income  student_No  student_Yes\n",
       "0   729.526495  44361.62507           1            0\n",
       "1   817.180407  12106.13470           0            1\n",
       "2  1073.549164  31767.13895           1            0\n",
       "3   529.250605  35704.49394           1            0\n",
       "4   785.655883  38463.49588           1            0"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2=  pd.get_dummies(X2, columns = {\"student\"})\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7500, 4), (2500, 4))"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size = 0.25)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96719999999999995"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regr = LogisticRegression()\n",
    "logistic_regr.fit(X_train, y_train)\n",
    "logistic_results = logistic_regr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, logistic_results)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original 75-25 split without including student data had an accuracy of .95. So there is a negligible improvement when including the student infromation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6 Problem 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Private</th>\n",
       "      <th>Apps</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene Christian University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1660</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelphi University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2186</td>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adrian College</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1428</td>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agnes Scott College</td>\n",
       "      <td>Yes</td>\n",
       "      <td>417</td>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alaska Pacific University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>193</td>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0 Private  Apps  Accept  Enroll  Top10perc  \\\n",
       "0  Abilene Christian University     Yes  1660    1232     721         23   \n",
       "1            Adelphi University     Yes  2186    1924     512         16   \n",
       "2                Adrian College     Yes  1428    1097     336         22   \n",
       "3           Agnes Scott College     Yes   417     349     137         60   \n",
       "4     Alaska Pacific University     Yes   193     146      55         16   \n",
       "\n",
       "   Top25perc  F.Undergrad  P.Undergrad  Outstate  Room.Board  Books  Personal  \\\n",
       "0         52         2885          537      7440        3300    450      2200   \n",
       "1         29         2683         1227     12280        6450    750      1500   \n",
       "2         50         1036           99     11250        3750    400      1165   \n",
       "3         89          510           63     12960        5450    450       875   \n",
       "4         44          249          869      7560        4120    800      1500   \n",
       "\n",
       "   PhD  Terminal  S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n",
       "0   70        78       18.1           12    7041         60  \n",
       "1   29        30       12.2           16   10527         56  \n",
       "2   53        66       12.9           30    8735         54  \n",
       "3   92        97        7.7           37   19016         59  \n",
       "4   76        72       11.9            2   10922         15  "
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college = pd.read_csv(\"College.csv\")\n",
    "college.shape\n",
    "college.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = college.drop(\"Apps\", axis=1)\n",
    "y = college[\"Apps\"]\n",
    "X = X.loc[:, \"Private\":\"Grad.Rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Private</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Private  Accept  Enroll  Top10perc  Top25perc  F.Undergrad  P.Undergrad  \\\n",
       "0     Yes    1232     721         23         52         2885          537   \n",
       "1     Yes    1924     512         16         29         2683         1227   \n",
       "2     Yes    1097     336         22         50         1036           99   \n",
       "3     Yes     349     137         60         89          510           63   \n",
       "4     Yes     146      55         16         44          249          869   \n",
       "\n",
       "   Outstate  Room.Board  Books  Personal  PhD  Terminal  S.F.Ratio  \\\n",
       "0      7440        3300    450      2200   70        78       18.1   \n",
       "1     12280        6450    750      1500   29        30       12.2   \n",
       "2     11250        3750    400      1165   53        66       12.9   \n",
       "3     12960        5450    450       875   92        97        7.7   \n",
       "4      7560        4120    800      1500   76        72       11.9   \n",
       "\n",
       "   perc.alumni  Expend  Grad.Rate  \n",
       "0           12    7041         60  \n",
       "1           16   10527         56  \n",
       "2           30    8735         54  \n",
       "3           37   19016         59  \n",
       "4            2   10922         15  "
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "      <th>Private_No</th>\n",
       "      <th>Private_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accept  Enroll  Top10perc  Top25perc  F.Undergrad  P.Undergrad  Outstate  \\\n",
       "0    1232     721         23         52         2885          537      7440   \n",
       "1    1924     512         16         29         2683         1227     12280   \n",
       "2    1097     336         22         50         1036           99     11250   \n",
       "3     349     137         60         89          510           63     12960   \n",
       "4     146      55         16         44          249          869      7560   \n",
       "\n",
       "   Room.Board  Books  Personal  PhD  Terminal  S.F.Ratio  perc.alumni  Expend  \\\n",
       "0        3300    450      2200   70        78       18.1           12    7041   \n",
       "1        6450    750      1500   29        30       12.2           16   10527   \n",
       "2        3750    400      1165   53        66       12.9           30    8735   \n",
       "3        5450    450       875   92        97        7.7           37   19016   \n",
       "4        4120    800      1500   76        72       11.9            2   10922   \n",
       "\n",
       "   Grad.Rate  Private_No  Private_Yes  \n",
       "0         60           0            1  \n",
       "1         56           0            1  \n",
       "2         54           0            1  \n",
       "3         59           0            1  \n",
       "4         15           0            1  "
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.get_dummies(X, columns = [\"Private\"])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1937495.65076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "test_error = mean_squared_error(y_test, ols.predict(X_test))\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ridge Regression Model with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=(0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10), cv=5,\n",
       "    fit_intercept=True, gcv_mode=None, normalize=False, scoring=None,\n",
       "    store_cv_values=False)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = RidgeCV(alphas = (0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10), cv=5)\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1940897.56066\n"
     ]
    }
   ],
   "source": [
    "test_error = mean_squared_error(y_test, ridge.predict(X_test))\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lasso Regression with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=(0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10), copy_X=True, cv=5,\n",
       "    eps=0.001, fit_intercept=True, max_iter=1000, n_alphas=100, n_jobs=1,\n",
       "    normalize=False, positive=False, precompute='auto', random_state=None,\n",
       "    selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = LassoCV(alphas = (0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10), cv=5)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1954605.32366\n"
     ]
    }
   ],
   "source": [
    "test_error = mean_squared_error(y_test, lasso.predict(X_test))\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.69521372e+00  -1.25603372e+00   4.40146072e+01  -8.04604794e+00\n",
      "   7.96463516e-02   9.29587658e-02  -1.03174639e-01   1.50365606e-01\n",
      "   1.12976747e-01  -1.72720322e-02  -7.86928680e+00  -4.50398355e-01\n",
      "   8.53858113e+00  -1.49289917e+00   5.14422421e-02   7.47109360e+00\n",
      "   1.70751229e+02  -0.00000000e+00]\n",
      "There are 17 non-zero coefficients\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for num in range(len(lasso.coef_)):\n",
    "    if lasso.coef_[num]==0.0:\n",
    "        count+=1\n",
    "print(lasso.coef_)\n",
    "print(\"There are\",len(lasso.coef_)-count, \"non-zero coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First doing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_reduced_train = pca.fit_transform(scale(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 32.14,  58.69,  65.4 ,  71.14,  76.35,  80.96,  84.44,  87.79,\n",
       "        90.86,  93.54,  95.7 ,  97.29,  98.34,  99.01,  99.52,  99.84,\n",
       "       100.  , 100.  ])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems like the first 9 components explain most of the variance in the dataset (more than 90%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(X_reduced_train)\n",
    "kf_10 = KFold(n_splits = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse = []\n",
    "regr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating PCA with only the intercept (no principal components in regression)\n",
    "score = -1*cross_val_score(regr, np.ones((n,1)), y_train.ravel(), cv=kf_10, scoring=\"neg_mean_squared_error\").mean()\n",
    "mse.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(pca.components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 18 principal components in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_reduced = pca.fit_transform(scale(X), y)\n",
    "for i in np.arange(1, 19):\n",
    "    score = -1*cross_val_score(regr, X_reduced_train[:,:i], y_train.ravel(), cv=kf_10, scoring=\"neg_mean_squared_error\").mean()\n",
    "    mse.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBTMz6+akYGZm3ZwUzMys2/8HF9p1pbsTUYUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b5b87b9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(mse, '-v')\n",
    "ax1.set_title(\"Principal Components Regression\")\n",
    "ax1.set_xlabel(\"Nujmber of principal components in regression\")\n",
    "ax1.set_ylabel('MSE')\n",
    "# ax2.plot(range(0, 19), mse, '-v')\n",
    "# ax2.set_title('Intercept excluded from plot')\n",
    "\n",
    "# for ax in fig.axes:\n",
    "#     ax.set_xlabel('Number of principal components in regression')\n",
    "#     ax.set_ylabel('MSE')\n",
    "#     ax.set_xlim((-0.2,5.2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above graph, the lowest MSE is when M=18. But this is almost same as doing regular least squares regression. There isn't any reduction going on.\n",
    "\n",
    "So, I decided to calculate the MSE for a value of M which would do some substantial reduction of the data. For M=6 till M=14, the MSE almost stays constant (there isn't much variation). I therefore decided on M=6 as if the MSE doesn't vary much, a lower dimension is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6243276.3274332006"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced_test = pca.transform(scale(X_test))[:,:7]\n",
    "# Train regression model on training data\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_reduced_train[:,:7], y_train)\n",
    "# Prediction with test data\n",
    "pred = regr.predict(X_reduced_test)\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This MSE is the same magnitude as that of the Linear, Ridge, and Lasso regression models. However, it is substantial greater than them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Partial Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = []\n",
    "kf_10 = KFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n",
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n",
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n",
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n",
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n",
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n",
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n",
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n",
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n",
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n",
      "C:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:319: UserWarning: X scores are null at iteration 17\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXJwsJW9hXISyCrIpi2Cwqilql7hWFYq2t\nS22r2Nb21lbrUu/93Wt7e1urttaFUlvFpa51XwpiVfZNZF8TtiQkQgJJIMv398c5E0LIBmTmnEne\nz8cjj8zMOTPnkzMn85nv93y/n2POOURERAASgg5ARETCQ0lBREQqKSmIiEglJQUREamkpCAiIpWU\nFEREpFJcJgUzm2FmOWa2sgHr/s7Mlvk/68xsTyxiFBGJRxaP8xTM7CxgH/C0c274UTzvNuA059x3\nohaciEgci8uWgnNuLpBf9TEzO9HM3jGzxWb2sZkNruGpU4FZMQlSRCQOJQUdQCN6HLjFObfezMYA\nfwTOjSw0sz5AP+BfAcUnIhJ6TSIpmFkb4AzgRTOLPJxSbbUpwD+cc+WxjE1EJJ40iaSA1w22xzl3\nah3rTAF+EKN4RETiUlyeU6jOOVcAbDazyQDmGRFZ7p9f6AB8FlCIIiJxIS6TgpnNwvuAH2Rm28zs\nBmAacIOZLQe+AC6r8pQpwHMuHodaiYjEUFwOSRURkeiIy5aCiIhER9RONJvZDOBiIKemCWZm9lO8\nLp9IHEOALs65/OrrVtW5c2fXt2/fRo5WRKRpW7x48W7nXJf61ota99HRzDo2s0uAHznnzq1rPYCM\njAy3aNGiRopSRKR5MLPFzrmM+taLWvdRTbOO66CZxiIiIRD4OQUzawVcCLxUxzo3m9kiM1uUm5sb\nu+BERJqZwJMCcAnwSV3nEpxzjzvnMpxzGV261NslJiIixygMSWEK6joSEQmFQJOCmbUDzgZeCzIO\nERHxRHNI6ixgAtDZzLYB9wLJAM65x/zVrgDec87tj1YcIiLxbNJDH7NqZ8ERjw/tkcZbt5/Z6NuL\nWlJwzk1twDozgZnRikFEJN6NTG/P+pxCSssPTR9ITjRG9ukQle2F4ZyCiIjUYvrEgSQcuiQAAIlm\nTJ84ICrbU1IQEQmxrmmpXDnyhMr7yYnGVRm96do2NSrbU1IQEQm5r4/sVXk7mq0EUFIQEQm9/Qe9\nC0YaRLWVAEoKIiKhl5lfBMCI3u2j2kqApnM5ThGRJisrv4gWSQm8/L0zSEiw+p9wHNRSEBEJucy8\nInp3aBn1hABKCiIioZeZX0SfTq1jsi0lBRGREHPOkZVfRHrHVjHZnpKCiEiI7SkqpfBAGb2VFERE\nJDLySC0FERFRUhARkUMiSaF3x5Yx2Z6SgohIiGXmFdG5TQqtWsRmWpmSgohIiGXmF5Eeo1YCKCmI\niIRaZgyHo4KSgohIaB0sq2Dn3mIlBRERgR17iqlwxGyOAigpiIiEVqyHo4KSgohIaFUmhU5KCiIi\nzV5WfhEtEhPoFsWL6lSnpCAiElKZ+UX06hibktkRSgoiIiEV6+GooKQgIhJKzjky85QUREQE2Fvs\nlcxWUhARkUCGo4KSgohIKAUxHBWUFEREQqmyZHYHJQURkWYvK7+Izm1a0DolNiWzI5QURERCaGte\nUUxrHkUoKYiIhFAQcxRASUFEJHRKyyvYsSe2JbMjlBREREImiJLZEVFLCmY2w8xyzGxlHetMMLNl\nZvaFmX0UrVhEROJJUHMUILothZnAhbUtNLP2wB+BS51zw4DJUYxFRCRuNMmk4JybC+TXsco3gJed\nc5n++jnRikVEJJ5kRkpmp8WuZHZEkOcUTgI6mNkcM1tsZtfVtqKZ3Wxmi8xsUW5ubgxDFBGJvaz8\nInp1aEliDEtmRwSZFJKA04GvAV8FfmlmJ9W0onPucedchnMuo0uXLrGMUUQk5jLzg5mjAMEmhW3A\nu865/c653cBcYESA8YiIhEIQJbMjgkwKrwHjzSzJzFoBY4DVAcYjIhK4vUWlFJTEvmR2RNSKapjZ\nLGAC0NnMtgH3AskAzrnHnHOrzewdYAVQATzpnKt1+KqISHNQWQivqSUF59zUBqzzG+A30YpBRCTe\nBDkcFTSjWUQkVIK6jkKEkoKISIhk5hfRqXUL2sS4ZHaEkoKISIhkBTgcFZQURERCJaiS2RFKCiIi\nIVFaXsH2gEpmRygpiIiExM49JZRXOCUFEREJfo4CKCmIiIRG0MNRQUlBRCQ0MvOLSE40ugdQMjtC\nSUFEJCS8ktmtAimZHaGkICISEkGWzI5QUhARCQlvjkLLQGNQUhARCYG9RaXsLS4NdDgqKCmIiIRC\n1pfBVkeNUFIQEQmBMMxRACUFEZFQUFIQEZFKmflFdGiVTFpqcqBxKCmIiIRAVsDVUSOUFEREQiAM\ncxRASUFEJHBl5RVs/7KYPgHWPIpQUhARCdjOvSWUBVwyO0JJQUQkYFkhGXkESgoiIoHbmh+OiWug\npCAiErjM/CKSEowe7YKtewRKCiIigcvML6JXh5aBlsyOUFIQEQlYVkiGo4KSgohI4DJDMnENlBRE\nRAK1t7iUPUXBl8yOUFIQEQlQVohGHoGSgohIoMI0RwGUFEREAhUpmZ0eghIXoKQgIhKozPwi2oeg\nZHaEkoKISIDCNPIIopgUzGyGmeWY2cpalk8ws71mtsz/uSdasYiIhFWY5ihAdFsKM4EL61nnY+fc\nqf7Pr6IYi4hI6JRXOLZ9Wdw8WgrOublAfrReX0Qk3u3cWxyaktkRQZ9TOMPMVpjZ22Y2rLaVzOxm\nM1tkZotyc3NjGZ+ISNRkhmyOAgSbFJYA6c65U4CHgVdrW9E597hzLsM5l9GlS5eYBSgiEk1hm7gG\nASYF51yBc26ff/stINnMOgcVj4hIrB0qmZ0adCiVAksKZtbdzMy/PdqPJS+oeEREYm1rXhEndGhJ\nUmLQPfmHJEXrhc1sFjAB6Gxm24B7gWQA59xjwFXA98ysDCgGpjjnXLTiEREJm6yQzVGAKCYF59zU\nepY/AjwSre2LiIRdZn4RF53cI+gwDhOeNouISDNSUFLKlyEqmR2hpCAiEoAwjjwCJQURkUAoKYiI\nSKXMkF1HIUJJQUQkAJn5RbRrmUy7luEomR2hpCAiEoDM/HAVwotQUhARCUAY5yiAkoKISMx5JbPD\ndR2FiDqTgpldW+X2V6otuzVaQYmINGW7CkooLQ9XyeyI+loKP65y++Fqy77TyLGIiDQLmXnhHI4K\n9ScFq+V2TfdFRKQBwjpHAepPCq6W2zXdFxGRBsjMLyIxwejRPjwlsyPqK4g32MxW4LUKTvRv49/v\nH9XIpEmb9NDHrNpZcMTjQ3uk8dbtZwYQkUjsZOYX0bN9KskhKpkdUV9SGBKTKKTZGZnenvU5hZSW\nH2pwJicaI/t0CDAqkdjIDOlwVKin+8g5t7XqD7APGAl09u+LHJPpEwdi1U5LOQeXn9oTXVZDmjov\nKbQOOowa1dlSMLM3gDudcyvNrAfedZUX4XUlPe6c+30sgpSmZ2124REf/mUVjqse+4xuaSmM69+J\nsf07Me7ETqR3bIV/kT6RuFdYUkr+/oOhbSnU133Uzzm30r/9beB959x1ZtYW+ARQUpCjNmtBJne/\nupK+HVuRtaeYg2UVpCYl8Lcbx7Auu5DPNubx7w27eXXZDgB6tktl7ImdKhNFZMKPzktIPMrKLwbC\nOfII6k8KpVVuTwSeAHDOFZpZRdSikiaposLx4Ltr+PNHmzjrpC48+o3TePDtNTyzIJOrMnozqm9H\nRvXtyLQxfXDOsSFnH/M25fHZpjzmrM3l5SXbAejVoSXj+neifatkkhNN5yUkrmSGeDgq1J8Usszs\nNmAb3rmEdwDMrCX+9ZZFGqL4YDk/fmEZb6/cxbQx6dx/6TCSEhOYPnEg63L2MX3igMPWNzMGdmvL\nwG5t+ea4vlRUONblFDJvo5ck3l+dzZ6i0iO2k2h2xGuJhEmY5yhA/UnhBuBXwHnANc65Pf7jY4G/\nRDMwaTpyCku46enFrNi2h7u/NoQbxverPEfQNS2VF747rt7XSEgwBndPY3D3NK7/Sj8qKhyrdxVw\nz2tfsHjrlwAkJRpXZfSma9vwjf0WicjMLyItNYl2rcL5vbrOpOCcywFuqeHx2cDsaAUlTce67EK+\n/ZeF5O0/wGPXns5Xh3VvlNdNSDCG9WzHn6aNZPyvZ3OwrILycsdNZ/ZrlNcXiZbM/CLSO4WzlQD1\njz56va7lzrlLGzccaUo+Xp/L9/++hNQWibzw3XGc0qt9o2+ja1oqV5/ei2fmZ+KAhz5cz28nj9Bo\nJQmtrPwiBvdoG3QYtaqv+2gckAXMAuajekfSQJERRgO7tuGp60dxQvuWUdtW5LzEKSe048l/b2Zs\nv05cPap31LYncqy8ktnFnD+sW9Ch1Kq+pNAdOB+YCnwDeBOY5Zz7ItqBSXyqOsLo7JO68Mg3TqNt\nanT7TiPnJcorHGt2FfLL11Zycq92DOmRFtXtihyt7IISDpZXhPYkM9Q/o7ncOfeOc+5beCeXNwBz\ndC0FqUnxwXJ+8OwS/vzRJq4dm85T38qIekKoKjHB+P2UU2nXMpnvP7OEwpIjRyeJBCnsw1GhAVde\nM7MUM7sS+DvwA+APwCvRDkziS05hCVOemMc7X+zi7q8N4YHLhpMUQLGvzm1SeHjqaWTmF3HnS5+r\nZIaESjwkhfpOND8NDAfeAu6vMrtZmrHaZhIb8Odvns4FjTTC6FiN6d+Jn1wwiAffWcOYeR25blzf\nQOMRicjKLyLBoGcUz7Edr/q+yl0LDARuBz41swL/p9DMjvxUkGZhZHp7khOPHHNw0fDugSeEiO+e\n1Z9zB3flgTdWsTxrT/1PEIkBr2R2y1CWzI6o75xCgnOurf+TVuWnrXNOZ/GaqekTB5JQbchnSlIC\n9102LKCIjpSQYPx28gi6tk3lB88uYW8Ns59FYi3MJbMjwpuuJLS6pqUytMrInuREY3IIZxJ3aN2C\nR75xGtkFJdzx4nKdX5DAZSkpSFPjnOM3765hadYeEvzGQpjrDZ2W3oFfTBrCB6uzeeLjTUGHI83Y\n/gNl7N53sLLKb1gpKUiDVVQ47nv9Cx6dvZGpo3szdXQ6ZoS+3tD1Z/TlouHdefCdtSzakh90ONJM\nZX0Z/pFHUP/ktWNmZjOAi4Ec59zwOtYbBXwGTHHO/SNa8TQFQV4/oKy8gv/4xwpeXrqdm8/qz88v\nGkxu4QHW11DhNGzMjAevOoVVD/+bW59dypvTx9OpTUrQYUkzszXPSwp9Qlz3CKLbUpgJXFjXCmaW\nCDwIvBfFOJqMmkb9xOL6AQfKyvn+M0t4eel2fnLBSfz8osGYWeVM4jC3EiLSUpN59BsjyS86yA+f\nX0ZFhc4vSGyFvWR2RNSSgnNuLlBfW/024CUgJ1pxNCU1jfqJdn/+/gNl3DBzEe+tyua+S4Zy67kD\n47bY3PAT2nHfJcP4eP1uHp29IehwpJnJzC+ibWoS7VqGs2R2RGDnFMzsBOAK4E9BxRBvuqalcvEp\nPQ577LT0DnRuHZ2ukL1FpXzzqfl8unE3/zt5BNd/Jf7LUk8d3ZvLT+3J7z5Yx6cbdgcdjjQjkeGo\nYf9SFbVzCg3we+BnzrmK+naSmd0M3AyQnp4eg9DCyTlHTsGByvtm8NmmPL7+2Kfcf+mwRi1NnVt4\ngOtmLGBjzj7+OG0kFw7vUf+T4oCZ8V9XnMzKHQVMf24Zb00fT9e08Hd/1UfXqw6/zPwiBnULb8ns\niCCTQgbwnJ8QOgOTzKzMOfdq9RWdc48DjwNkZGQ0287g15bt4OMNuxmZ3p6lWXuYNjqd09I78N9v\nr+GyRz/hmoze/PSrg477JOr2PcVc++R8du0t4anrMzhzYJdG+gvCoXVKEn+aNpJLH/mE22Yt5Zkb\nxwRSp6kxjUxvz/qcwiZxveqwJLjGjKOiwrEtv5jzh4S3ZHZEYEnBOVfZF2FmM4E3akoI4skpKOHe\n179gZHp7Hv3GSG5/fhnTzxtI17apnD+sG3/4YD0zP93CW5/v5I4LBjFtTPoxfdBtyt3HtU/Op/BA\nGX+7YTQZfTtG4a8J3sBubfmvK4bz4xeWM+Cut49YHm/fsKdPHMiLi7cBh5JCQojnj9QlLAmuMePI\nLvRKZod9jgJE8ZyCmc3CG2o6yMy2mdkNZnaLmR1xeU+pm3OOX7yykpLScn4zeQQ92rc8bNRPWmoy\nd188lLdvP5OTe7Xj3te/4OKH/838TXlHtZ1VOwq4+s+fcaCsglk3jW2yCSHiypG9GNCl9RGPx+M3\n7K5pqZw/9PBvoQfKKrjuqQX8z9trmLcpj9LyioCiOzpBDKiIdhyZefEx8gii2FJwzk09inWvj1Yc\nTcFry3bwweps7po0hBO7tKl1vYHd2vL3G8bwzspd/Oebq7nm8XlcOqInv5g0hO7t6u43X7w1n2//\nZSGtU5L4+41j6txOUzLj26M4+9dzqNonGeYZ2rVZs6uAT9YfOnHeIimBm8/qz6It+Tz58SYe+2gj\nbVOSGD+wMxMGdeHsk7rWe0wE4UBZOXPW5ZKSlMCBMi+JGXDBsG4xH/pcdLCcdi2TySk8dB6vtKKC\nO15Yztj+nRjTryOn9GpPi6T6v1vHQ8nsCIu3ejAZGRlu0aJFQYcRMzkFJZz/u7mc2KU1L95yBokJ\nDRu5UHywnD99tJHHPtpIUoJx67kDuGF8P1KSEo9Y99/rd3PT04volpbC328cQ68O4T9wG9P0WUt4\nffnOyvt9O7XijgsGcfagLqTF8CJBx2rVjgKmPTmPlKRERvfrwD9X7GTamD785+XenNHCklI+2ZDH\nnLU5zFmby66CEgAGd2/LhEFdOWdQF0b26cBlj3wSWF9+QUkps+ZnMuOTzWQXHGBg1zZsydtf2XWT\nnGD88PyTuPms/lGvMFpe4fjLJ5v53/fWkphgHCitoKzCkZxoXH7qCSzftod12fsASE1O4PQ+HRjT\nz0sSI3q3JzX50P9YWM6PAJjZYudcRr3rKSmEl3OOm55ezMfrc3nr9jOP6dt7Zl4RD7y5ivdXZdOv\nc2vKK1zlt5aqUpIS+Phn58TFRLTGllNQwpm/ns2BsgoSDdqmJrOnuJTkRGNs/06cN6Qb5w3tFtXr\nTB+rldv3cu1T82mZnMism8bSqkUit85ayiPfOK3G99I5x9rsQuaszWXO2hwWbfmSsgpH25Qk2rdO\nZseeYqr2MiUnGteMSq9MMI0tu6CEGZ9s5tl5mRQeKGP8gM589+z+jB/QmV++upJnFmTy9ZEnUHyw\ngjc/38mwnmn8+qpTGNazXVTi2ZBTyE//sYKlmXs4b0hX/uuKk3n4w/U8syDzsESbv/8gCzbnMW9T\nPvM357NmVwHOeS2003q3Z0z/Tozt15HXl+/gpSXbjjgvEc19WhslhSbg1aXb+eHzy7hr0hBuOqv/\ncb3WnLU5/Oqfq9i0ez8Gh3WXGDA5oxe/vmrEcW0jnt39yueV//j3XzqMpZlf8v6qbN5fnc2m3P2A\n9+3uvKHdOH9IN4afkFY53jyob4Ofb9vLtCfn0TY1mVk3jSX9GMoneK2I3cxZm8uHq3PI3XfgsOXJ\nicY/bxvP4O6NWyl/Q04hj8/dxCtLt1Ne4fjaKT357ln9GX7CoQ/7nIKSwxLcOyt3cverX7Cn6CC3\nnH0it00cUGPL91iUllfw+NxNPPTBelqnJHLfpcO4dERPzOyIOGqyt6iUBVvymb8pj/mb8/lix14q\nHCQlQLmDqh+zqUkJzA3gC5iSQpw71m6juhwsq+DhD9fzcLXZvM25lRBR1z/+xtx9fLg6m/dXZbN4\n65dUOOielsp5Q7ty3pBuvPvFLv6xOLbfBpdl7eGbT82nXUsvITTGqBbnHLc+u5R3Vu6kvNrHwold\nWjPa7yIZ3a/jMV85bNGWfB77aBMfrM4mNTmBqzN6c+P4/g1OaHuKDvLAG6t5ack2BnZtw6+vOoXT\n0o9vUMCqHQX8x0vLWbm9gK+d3IP7Lh1Gl7bHN6y7oKSURVvymb8pn38s3kbe/oNAcK0EUFKIa43R\nbVSXH7+wjFeXbqfCBXuQxqO8fQeYvTaX91ftYu663RSXltMy2TspWhGjb4NLMr/kW08toH1rLyE0\n5jmgql1pKUkJ/PHakazP3seCzfks3JJPYUkZAL06tKzsRx/dryN9OnkzdWtrNfXu0JKuaaks3vol\nHVolc924vlw3rs8xz6mZvTaHu17+nF0FJXznK/2444JBtGxxdK2Gg2UVPDJ7A3+cvYH2rZJ54LLh\nXHRy40/SrLpPg2olQMOTQpCT16QWDR1tdKzuvHAwb67Y6fehx99ImyB1apPCVaf34qrTe1FSWs5n\nG/N4b1U2ryzdRkmp1xmfmABXnd4rKv/4i7fm860ZC+nUpgWzbhrb6Nf67ZqWyuTTe/HMgkwmZ/Rm\n4uBuTBzcjVvOPpHyCseaXQUs2Ox9A569NoeXlmzzntc2hdH9OtI6JZGkBKOsWsHBrC+LccD9lw5j\nckYvWrU4vo+ecwZ15d0fncWD76zhyX9v5v3V2fzPlacw7sRODXr+im17+OmLK1ibXciVp53ALy8e\nSofWLY4rptpU3adhLzMPaimETk5hCef/X+N2G9Wkah+6WgnHb9eeYs78zezKLqRB3dtw7yXDOOPE\nzo22jYVb8rl+xgK6pqUy66axURtS2pA+dPBatBtz9zF/c35looiMbKrKgAcuH86UUb2jMnP8s415\n3PnyCrbmFTFtTDp3XjSYtrWMGispLef3H6zn8bkb6do2lf935XDOHRz9WcYN3afRpO6jOOSc4+a/\nLeajdbm8HYVuo6rCcJA2NZFEe0b/TmzJK2L7nmLOHdyVOy8azEnHWfNm/qY8vj1zId3beQmhWwjr\nNTnnyMov5mcvLWf+5nz/RKsxZVRv/vOKk6O67eKD5fz2vbXM+GQz3dNSSUpMqHGUXYtE42C5Y+ro\n3vx80pC4GHLcWJQU4lBktNEvJg3m5rNODDocOUpVE21aajIzP93Co7M3sP9AGdeM6s2PzjvpmIrv\nfbpxNzfMXMQJHVry7E1jQp/Eg+xDX5L5Jf/xjxVsyNlHgkH1y2a0bpHIn7+ZwfiBjdeCixdKCnEm\nVt1GElv5+w/y8L/W8/d5W0lOTOCmM/tz81n9aZ3SsD71Tzbs5oa/LiS9YyueuXHscY+KiZUguycP\nlJXz4NtrmPHJlsMeT0wwPvzx2fTtfGRpk+agoUkhvktDNhHOOe56ZSXFfm0jJYSmo2PrFtx7yTA+\n+PHZnDOoKw99uJ4J/zuHZ+dnUlZPLaK563L5zsyF9O3Ummdvip+EAF7doFF9OwYyiCElKZF7LhnG\npJO7Vz6WlGBMHZ3ebBPC0VBSCIHXlu3g/VXZ/OSCk5pNzaHmpk+n1jw6bSQvf/8M+nRsxS9e+ZyL\nHvqYD1dnU1Nrfc7aHG58ehH9u7Th2ZvG0jnOrikdhku13nfJMFL8ukRJCRpl11AakhqwnMJDJbFv\nGH98s5Yl/Eamd+DFW8bx7hfZPPjOGm746yLG9u9IdsEBNu/ef8T6FRWOjlEaKtnUxdtQ0LBQUgiQ\nuo2aJzPjwuHdmTikK7MWZPL7D9aTv//gEeVHkhONUf2advnyaJs+cSDrcvaplXAU1H0UIHUbNW/J\niQlcN64vc346gW+N60P1TiRNLDx+YejGijdKCgFRt5FEpKUmc/9lw7nitJ4k+o3F5ERTl4cEQt1H\nMVJbTZi9xaXqNhIAfn7REN76fBflKj8iAVJLIUZGprcnObH6pf1gXCOWQZD4FjkxaoZaCRIYJYUY\nqel6r8mJCfo2KIcJcny/CCgpxEzXtFTOHHCoVZCkPmOpgU6MStB0TiEGig+W8+A7a/hgTU7lsMMk\n9RmLSAgpKUTZkswvueOF5WzevZ/rz+hLSWk5zy/KUitBREJJSSFKDpSV89AH63nso430aNeSZ28c\nwxkDOpNTUMKm3fvVShCRUFJSiIJVOwr48QvLWLOrkMmn9+KXlwytrNse6TMWEQkjJYVGVFZewZ/n\nbuL3H6yjXcsWPHldBucNjf5VnUREGouSQiPZlLuPO15cztLMPXzt5B48cPlwFTITkbijpHCcKioc\nT3+2hf95Zw0pSYk8NOVULh3REzPNUhaR+KOkcBy27ynmpy8u59ONeUwY1IUHv35KKK+dKyLSUEoK\nDVBb3aIEg5bJifz3lSczZVRvtQ5EJO4pKTTAyPT2rM8ppLT88OLGnduk8NL3zqB3x1YBRSYi0rhU\n5qIBaqpblJRg/PPW8UoIItKkKCk0QGX1Sv9+UqIxZXQ63drp/IGINC1KCg00bcyhK2OpbpGINFVR\nSwpmNsPMcsxsZS3LLzOzFWa2zMwWmdn4aMXSGD5anwuAoVr3ItJ0RbOlMBO4sI7lHwIjnHOnAt8B\nnoxiLMfFOcfzC7MY0asdo/qp1r2INF1RG33knJtrZn3rWL6vyt3WcMR1y0Nj3qZ8Nu/ez28nj+Dr\np/cKOhwRkagJ9JyCmV1hZmuAN/FaC7Wtd7PfxbQoNzc3dgH6nluYSdvUJCad3CPm2xYRiaVAk4Jz\n7hXn3GDgcuCBOtZ73DmX4ZzL6NKlS+wCBPYUHeTtlbu44rQTaNkiMabbFhGJtVCMPnLOzQX6m1no\nrmL/8pLtHCyrYMqo9KBDERGJusCSgpkNML8uhJmNBFKAvKDiqYlzjucWZjKiVzuG9kwLOhwRkaiL\n2olmM5sFTAA6m9k24F4gGcA59xjwdeA6MysFioFrnHOhOtm8JHMP67L38f+uODnoUEREYiKao4+m\n1rP8QeDBaG2/MTy/MJNWLRK59NSeQYciIhIToTinEEaFJaX8c/lOLjmlJ21SVDdQRJoHJYVavL58\nB8Wl5UwZ3TvoUEREYkZJoRbPLchicPe2nNq7fdChiIjEjJJCDVZu38vn2/fqwjki0uwoKdTguYWZ\npCQlcMVpKmkhIs2LkkI1RQfLeG3pDiad3IN2rZKDDkdEJKaUFKp5c8VOCg+UMWWUTjCLSPOjpFDN\ncwuz6N+lNaP7dQw6FBGRmFNSqGJddiGLt36pE8wi0mwpKVTx/MIskhONK0fqBLOINE9KCr4DZeW8\nvGQb5w8z/LW3AAAP7UlEQVTtRuc2KUGHIyISCCUF37tfZPNlUalKZItIs6ak4HtuQSa9OrRk/IDQ\nXdJBRCRmlBSArXn7+XRjHtdk9CYhQSeYRaT5UlLAG4aaYDA5Q3MTRKR5a/ZJobS8ghcXbePcwV3p\n3i416HBERALV7JPCh6tz2L3vgE4wi4igpMBzCzPplpbChEFdgg5FRCRwzTopbN9TzEfrcrk6ozdJ\nic16V4iIAM08Kby4KAuAq3WCWUQEaMZJobzC8cLCLMYP6Ezvjq2CDkdEJBSabVKYuz6XHXtLmDpa\nJ5hFRCKabVJ4bkEmnVq34Lwh3YIORUQkNJplUsgpLOHD1Tl8/fRetEhqlrtARKRGzfIT8R+Lt1FW\n4bhGV1cTETlMs0sKFRWO5xdmMbpfR07s0ibocEREQqXZJYV5m/LYmlfE1NFqJYiIVNfsksKshVmk\npSZx0fAeQYciIhI6zSop5O8/yLsrd3HlyF6kJicGHY6ISOgkBR1AtE166GNW7Sw47LGZn25hweZ8\n3rr9zICiEhEJpybfUhiZ3p7kxMMvnJOcaIzs0yGgiEREwqvJJ4XpEweSYIcnhUQzpk8cEFBEIiLh\n1eSTQte0VCaf3osk/zKbyYnGVRm96dpWF9QREakuaknBzGaYWY6Zraxl+TQzW2Fmn5vZp2Y2Ilqx\nTJ84kEQ/KaiVICJSu2i2FGYCF9axfDNwtnPuZOAB4PFoBRJpLZihVoKISB2iNvrIOTfXzPrWsfzT\nKnfnAb2iFQt4rYV1OfvUShARqUNYhqTeALxd20Izuxm4GSA9/dhKXXdNS+WF7447pueKiDQXgZ9o\nNrNz8JLCz2pbxzn3uHMuwzmX0aWLrqUsIhItgbYUzOwU4EngIudcXpCxiIhIgC0FM0sHXga+6Zxb\nF1QcIiJySNRaCmY2C5gAdDazbcC9QDKAc+4x4B6gE/BH8yaXlTnnMqIVj4iI1C+ao4+m1rP8RuDG\naG1fRESOnjnngo7hqJhZLrD1GJ/eGdjdiOFEU7zEqjgbX7zEqjgbV7Tj7OOcq3ekTtwlheNhZovi\npYsqXmJVnI0vXmJVnI0rLHEGPiRVRETCQ0lBREQqNbekELX6SlEQL7EqzsYXL7EqzsYVijib1TkF\nERGpW3NrKYiISB2UFEREpFKTTApmdqGZrTWzDWZ2Zw3Lzcz+4C9fYWYjA4ixt5nNNrNVZvaFmd1e\nwzoTzGyvmS3zf+6JdZxVYtniXxBpmZktqmF5GPbpoCr7apmZFZjZD6utE8g+remiU2bW0czeN7P1\n/u8aLxxe3/Eco1h/Y2Zr/Pf2FTNrX8tz6zxOYhDnfWa2vcr7O6mW58Zsn9YS5/NVYtxiZstqeW7M\n9mcl51yT+gESgY1Af6AFsBwYWm2dSXilug0YC8wPIM4ewEj/dltgXQ1xTgDeCHqf+rFsATrXsTzw\nfVrDcbALb8JO4PsUOAsYCays8tivgTv923cCD9byd9R5PMco1guAJP/2gzXF2pDjJAZx3gf8pAHH\nRsz2aU1xVlv+W+CeoPdn5KcpthRGAxucc5uccweB54DLqq1zGfC088wD2ptZj1gG6Zzb6Zxb4t8u\nBFYDJ8QyhkYW+D6tZiKw0Tl3rLPfG5Vzbi6QX+3hy4C/+rf/Clxew1Mbcjw3qppidc6955wr8+9G\n/aJYDVHLPm2ImO7TuuI0r/Db1cCsaG3/aDXFpHACkFXl/jaO/LBtyDox41+h7jRgfg2Lz/Cb7G+b\n2bCYBnY4B3xgZov9ix5VF6p9Ckyh9n+0sOzTbs65nf7tXUC3GtYJ234F+A61XxSrvuMkFm7z398Z\ntXTJhWmfnglkO+fW17I85vuzKSaFuGJmbYCXgB865wqqLV4CpDvnTgEeBl6NdXxVjHfOnQpcBPzA\nzM4KMJY6mVkL4FLgxRoWh2mfVnJeX0Hox4eb2V1AGfBMLasEfZz8Ca9b6FRgJ17XTJhNpe5WQsz3\nZ1NMCtuB3lXu9/IfO9p1os7MkvESwjPOuZerL3fOFTjn9vm33wKSzaxzjMOMxLLd/50DvILXBK8q\nFPvUdxGwxDmXXX1BmPYpkB3pYvN/59SwTmj2q5ldD1wMTPOT2BEacJxElXMu2zlX7pyrAJ6oZfuh\n2KdmlgRcCTxf2zpB7M+mmBQWAgPNrJ//jXEK8Hq1dV4HrvNHzIwF9lZpxseE35f4FLDaOfd/tazT\n3V8PMxuN937F/Ap1ZtbazNpGbuOddFxZbbXA92kVtX77Css+9b0OfMu//S3gtRrWacjxHHVmdiHw\nH8ClzrmiWtZpyHESVdXOY11Ry/ZDsU+B84A1zrltNS0MbH/G8qx2rH7wRsKswxthcJf/2C3ALf5t\nAx71l38OZAQQ43i87oIVwDL/Z1K1OG8FvsAbHTEPOCOg/dnfj2G5H08o96kfR2u8D/l2VR4LfJ/i\nJamdQCleH/YNeBeZ+hBYD3wAdPTX7Qm8VdfxHECsG/D64SPH6mPVY63tOIlxnH/zj78VeB/0PYLe\npzXF6T8+M3JcVlk3sP0Z+VGZCxERqdQUu49EROQYKSmIiEglJQUREamkpCAiIpWUFEREpJKSQpSY\nmTOz31a5/xMzu6+RXnummV3VGK9Vz3Ymm9lqM5t9HK/xpJkNPcbnfnoc251jZoFfBD3azOzyo9m/\nZpZhZn84ym1EKnWuMLP3zKx7lcc7V1u3m5m9YWbLzasA/NbRbCtIZvYrMzsv6DiCpqQQPQeAKwOc\nLVsjfxZlQ90A3OScO+cYt5XonLvRObfqWJ7vnDvjWJ7XzFwONDgpOOcWOeemH8N2znFeaZBFwC/q\nWO9XwPvOuRHOuaF41V9r5U92PK7PoaM8pmvlnLvHOfdBY7xWPFNSiJ4yvGuu/qj6gurf9M1sn/97\ngpl9ZGavmdkmM/sfM5tmZgv8b2onVnmZ88xskZmtM7OL/ecnmlf3fqH/re67VV73YzN7HTjiA9rM\npvqvv9LMHvQfuwdvgt1TZvabautPMLO5ZvameTXpH4v8Y5vZPjP7rZktB8ZV/cbuL/sv/1vkPDPr\n5j/ezbwa/cv9nzNq2C+1be9P/n74wszur+9NMbNRZvapv50FZtbWzFLN7C/+PlhqZuf4615vZq+a\nd62DLWZ2q5n92F9nnpl19NebY2YPmVfzfqV5M6Uj10t41X8v5pnZKf7j95lXrG2O/z5PrxLftX5c\ny8zsz2aWWNu+8/fTpcBv/PVPNLPp/jf0FWb2XA1//wQze6O+OOowFxhQx/IeeBO0AHDOraghhr7+\n+/g03gzd3mZ2gZl9ZmZLzOxF82qCYWaTzLuOw2LzrtdRNfa/mdknwN/qOPZ7+MdO5L050193pn//\nczP7kb9u5f+lmU303+fP/X2U4j++xczu9+P83MwGN2CfxZdYzJBrjj/APiANrx56O+AnwH3u0EzG\nq6qu6/+eAOzB+8dKwavHcr+/7Hbg91We/w5eUh+I90+YCtwM3O2vk4L3ra6f/7r7gX41xNkTyAS6\nAEnAv4DL/WVzqGFmsv96JXgzLhOB9yN/D94s7aurrFv5Gv6yS/zbv64S6/N4BQHxX69dDfultu11\nrPK8OcAptcWOVzt/EzDKv5/m/813ADP8xwb7+yMVuB5vJm9bf//s5dDM6N9ViXkO8IR/+yz8uvl4\nBffu9W+fCyzzb98HfOq/R53xZmAnA0OAfwLJ/np/BK6rZ9/N5PBjaQeQ4t9uX8t790ZdcdTwnC34\nNf2BR/CvpUANtf6Br+Idw7OBu4CeNbxeX6ACGOvf74yXbFr7938G3OO/B1n4xy3ezOCqsS8GWvr3\nazv27+DQDPxE/708Ha81Q9X9FNmXVbZ7kv/401Xe6y3Abf7t7wNPBv1Z09g/ailEkfOqnj4NHE1z\nfaHzrrVwAG8K/nv+45/j/TNFvOCcq3Beyd1NeB9mF+DVH1qGV4a7E17SAFjgnNtcw/ZGAXOcc7nO\nq5f/DN4HW30WOK8efTneP+t4//FyvCJ/NTkIvOHfXlzl7zkXr7olzitmtvcotne1mS0BlgLDqLsr\nZRCw0zm30N9Wgf83jwf+7j+2BtgKnOQ/Z7ZzrtA5l4uXFP7pP179/ZjlP38ukGbelcnG45VdwDn3\nL6CTmaX567/pnDvgnNuNVwivG941IE4HFvrv4US8RFjXvqtuBfCMmV2L11qtT01x1GS2H1Ma8N+1\nvZhz7l0/5ifwjsmlZtalhlW3Ou+6G+BdlGko8Im/jW8Bffznb6py3FavZ/W6c67Yv13bsb8Q+LZ5\n5/NOdt61SzYB/c3sYfNqOlWvTjwI2OycW+ff/yuH/09EilfW9T7ErUbpi5M6/R6vXPNfqjxWht91\n53eDtKiy7ECV2xVV7ldw+PtVvT6Jw6s/dJv/j1nJzCbgtRQaU03bByjxP7hrUur8r1h4yeNojr8j\ntmdm/fBaYKOcc1+a2Uy8b3mN6Xjej4a+bmRfGPBX59zPa1i/ofvua3gfYJcAd5nZye7QxXEaGkdN\nzvETR72cc/nAs8CzfnfPWRz5RaHq8Wh439ynVl3BzE6tZ1PVX+OIY99/nbPw9stMM/s/59zTZjYC\nr1VzC95Fbr5T/19WKbLPjvYYjgtqKUSZ/w/yAt5J24gteN8IwesTTj6Gl55sZgnmnWfoD6wF3gW+\nZ15JbszsJPOqK9ZlAXC2mXX2+6+nAh81YPujzasymQBcA/z7GP6GiA+B7/kxJ5pZuwZuLw3vg2Gv\neecnLqpnO2uBHmY2yt9WW/NOUn4MTPMfOwlI99c9Gtf4zx+PVyF2b7XXnQDsdkdeM6OqD4GrzKyr\n/5yOZtannu0W4nWJRL5g9HbOzcbrgmkHtDnKv+O4mNm5ZtbKv90WOBGvO64u84CvmNkA/3mt/fdh\nLd43+r7+etfU8Ro1Hvv+/st2zj0BPAmMNG/wR4Jz7iXgbrxLZVa1FugbiQf4Jg37n2gSmlyWC6nf\n4lXnjHgCeM28k7HvcGzf4jPxPtDT8Pq5S8zsSbzm7BIzMyCXmi/xWMk5t9O8C5fPxvu29aZzrqYS\nztUtxOtfHuA/95Vj+BsibgceN7Mb8L59fQ/4rL7tOecqzGwpsAavD/iTujbinDtoZtcAD5tZS6AY\nr3zxH4E/mdnneK24651zB7xd2GAlfizJHPrWeR8ww8xWAEUcKpNdW3yrzOxu4D3/A74U+AFed1Zt\nngOe8E8ST8EbGNAO7738g3Nuz9H8EcdghZlV+LdfwKsG+oiZRVrDT0a662rjnMs171oNsyIndPHO\nD6wzs+8D75jZfrxjoDa1HfsTgJ+aWSneeb7r8K6y9hc7NOrpsJaZ/7/0beBF/0vDQuCxevZDk6Eq\nqXLU/G+9P3HOXdwUt3e0zGwOXnyLgo6lqTGzNs65ff4H/aPAeufc74KOqylT95GIhNlN/snjL/C6\nw/4ccDxNnloKIiJSSS0FERGppKQgIiKVlBRERKSSkoKIiFRSUhARkUr/HzPoy4edvwAZAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b5c1b7048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "for i in np.arange(1, 19):\n",
    "    pls = PLSRegression(n_components=i, scale=True)\n",
    "    pls.fit(X_train,y_train)\n",
    "    score = -1*cross_val_score(pls, X_test, y_test, cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "    \n",
    "plt.plot(np.arange(0, 19), np.array(mse[0:19]), '-v')\n",
    "plt.xlabel('Number of principal components in PLS regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above graph, the MSE is least when M=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLSRegression(copy=True, max_iter=500, n_components=10, scale=True, tol=1e-06)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls15 = PLSRegression(n_components = 10)\n",
    "pls15.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12402224.053573677"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, pls15.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pls7 = PLSRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = pd.read_csv(\"Boston.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = boston.loc[:, \"zn\":\"medv\"]\n",
    "y = boston[\"crim\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=(0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10), cv=5,\n",
       "    fit_intercept=True, gcv_mode=None, normalize=False, scoring=None,\n",
       "    store_cv_values=False)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = RidgeCV(alphas = (0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10), cv=5)\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.1696044869\n"
     ]
    }
   ],
   "source": [
    "test_error = mean_squared_error(y_test, ridge.predict(X_test))\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=(0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10), copy_X=True, cv=5,\n",
       "    eps=0.001, fit_intercept=True, max_iter=1000, n_alphas=100, n_jobs=1,\n",
       "    normalize=False, positive=False, precompute='auto', random_state=None,\n",
       "    selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = LassoCV(alphas = (0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10), cv=5)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.134735215\n"
     ]
    }
   ],
   "source": [
    "test_error = mean_squared_error(y_test, lasso.predict(X_test))\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying subset selection using feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([  3.29820000e-01,   1.96570000e-01,   8.64476000e+00,\n         2.90900000e-01,   7.50300000e-02,   4.12380000e-01,\n         1.10874000e+01,   1.43337000e+01,   2.29690000e-01,\n         5.05900000e-02,   6.11540000e-01,   1.84982000e+01,\n         8.30800000e-02,   6.86000000e-02,   9.55770000e-01,\n         2.28760000e-01,   8.24809000e+00,   8.52040000e-01,\n         2.59150000e-01,   6.41700000e-02,   1.02330000e+01,\n         5.47900000e-02,   1.10270000e-01,   3.76800000e-02,\n         8.71675000e+00,   2.98190000e-01,   2.63548000e+00,\n         5.44520000e-01,   5.87205000e+00,   8.15174000e+00,\n         4.26131000e+00,   4.59000000e-02,   8.38700000e-02,\n         4.64689000e+00,   5.40500000e-01,   2.61690000e-01,\n         8.05579000e+00,   1.71340000e-01,   5.69175000e+00,\n         1.19294000e+00,   8.89762000e+01,   1.35540000e-01,\n         2.07460000e-01,   3.53700000e-02,   7.02259000e+00,\n         5.66637000e+00,   1.35870000e-01,   6.12900000e-02,\n         1.90730000e-01,   1.61282000e+00,   4.81900000e-02,\n         1.00080000e-01,   2.54300000e-02,   4.81213000e+00,\n         4.29700000e-02,   4.55587000e+00,   2.10380000e-01,\n         1.34284000e+00,   2.72900000e-02,   1.43900000e-02,\n         2.44953000e+00,   3.56868000e+00,   1.30100000e-02,\n         2.36862000e+00,   3.57800000e-02,   1.29320000e-01,\n         2.98500000e-02,   2.06080000e-01,   4.34879000e+00,\n         6.26300000e-02,   5.34120000e-01,   4.29400000e-02,\n         3.76619000e+01,   1.00623000e+01,   3.87100000e-02,\n         3.04100000e-02,   6.72400000e-02,   9.82349000e+00,\n         1.20742000e+00,   6.15100000e-02,   1.54450000e-01,\n         8.98296000e+00,   4.11300000e-02,   1.06120000e-01,\n         6.37960000e-01,   5.60200000e-02,   1.67600000e-01,\n         3.69695000e+00,   7.89600000e-02,   2.39120000e-01,\n         1.62864000e+00,   3.84970000e+00,   2.81838000e+00,\n         4.37900000e-02,   4.89822000e+00,   1.64390000e-01,\n         1.27570000e-01,   5.20580000e-01,   1.80028000e+00,\n         1.39134000e+01,   2.31390000e+00,   5.73116000e+00,\n         4.87141000e+00,   5.29305000e+00,   2.50461000e+01,\n         9.18702000e+00,   5.37200000e-02,   2.21880000e-01,\n         4.22239000e+00,   6.32000000e-03,   4.01100000e-02,\n         8.37000000e-02,   9.88430000e-01,   9.92485000e+00,\n         4.68400000e-02,   1.30580000e-01,   3.51140000e-01,\n         1.36781000e+01,   9.84900000e-02,   8.87300000e-02,\n         9.72418000e+00,   1.62110000e-01,   7.25800000e-01,\n         6.71772000e+00,   8.01400000e-02,   6.28807000e+00,\n         8.81250000e-01,   4.92980000e-01,   7.95000000e-02,\n         1.40520000e-01,   9.06000000e-03,   7.72990000e-01,\n         3.58090000e-01,   1.41500000e-01,   3.52330000e-01,\n         3.39830000e-01,   6.89900000e-02,   1.04690000e-01,\n         1.70900000e-02,   6.71910000e-01,   3.15000000e-02,\n         7.16500000e-02,   1.25790000e-01,   4.54192000e+00,\n         3.30450000e-01,   3.65900000e-02,   3.03470000e-01,\n         9.60400000e-02,   2.83920000e-01,   3.54800000e-02,\n         4.07710000e-01,   9.37800000e-02,   1.10690000e-01,\n         2.00900000e-02,   6.04700000e-02,   1.88110000e+01,\n         5.02300000e-02,   1.22472000e+01,   1.06718000e+01,\n         8.24400000e-02,   1.43200000e-02,   1.32620000e-01,\n         6.23560000e-01,   9.51200000e-02,   9.23230000e+00,\n         2.33099000e+00,   6.90500000e-02,   7.83932000e+00,\n         1.65660000e+00,   4.56000000e-02,   3.22640000e-01,\n         8.70700000e-02,   7.01300000e-02,   1.38799000e+00,\n         7.88600000e-02,   2.87500000e-02,   1.00245000e+00,\n         4.52700000e-02,   3.70500000e-02,   2.45220000e-01,\n         2.15505000e+00,   1.13081000e+00,   1.15172000e+00,\n         7.97800000e-02,   2.48017000e+01,   1.59360000e-01,\n         5.82115000e+00,   9.16400000e-02,   8.49213000e+00,\n         3.87350000e-01,   5.78000000e-02,   3.46600000e-02,\n         4.09740000e+00,   1.87000000e-02,   4.15292000e+01,\n         1.23290000e-01,   5.50070000e-01,   2.49800000e-01,\n         5.20177000e+00,   9.10300000e-02,   6.64200000e-02,\n         9.25200000e-02,   7.84200000e-01,   3.67822000e+00,\n         6.39312000e+00,   3.40060000e-01,   7.40389000e+00,\n         1.46336000e+00,   2.89600000e-01,   1.81590000e-01,\n         7.36711000e+00,   1.58760000e-01,   1.42362000e+01,\n         6.12700000e-02,   8.82900000e-02,   9.51363000e+00,\n         5.11830000e-01,   5.18800000e-02,   2.37857000e+00,\n         6.29760000e-01,   2.53870000e-01,   3.69311000e+00,\n         3.49400000e-01,   1.42310000e-01,   3.18270000e-01,\n         3.53501000e+00,   3.41090000e-01,   1.83377000e+00,\n         7.05042000e+00,   6.96215000e+00,   2.99160000e-01,\n         7.87500000e-02,   2.20511000e+01,   1.83370000e-01,\n         5.42500000e-02,   9.29900000e-02,   6.56650000e-01,\n         1.27440000e-01,   5.66998000e+00,   1.22690000e-01,\n         1.77800000e-02,   7.35341000e+01,   7.15100000e-02,\n         3.50200000e-02,   9.06500000e-02,   4.42228000e+00,\n         5.37000000e-01,   1.33598000e+01,   4.03841000e+00,\n         3.47428000e+00,   1.50100000e-02,   1.44550000e-01,\n         8.22100000e-02,   5.82401000e+00,   1.02900000e-01,\n         3.44500000e-02,   1.31170000e-01,   2.86558000e+01,\n         4.33700000e-02,   7.52601000e+00,   1.74460000e-01,\n         2.24236000e+00,   5.26930000e-01,   2.14918000e+00,\n         9.17800000e-02,   2.18700000e-02,   5.78900000e-02,\n         2.51990000e-01,   6.44405000e+00,   2.25971000e+01,\n         6.63510000e-01,   8.40540000e-01,   4.66883000e+00,\n         1.23247000e+00,   2.24890000e-01,   2.68380000e-01,\n         7.85700000e-01,   1.41385000e+00,   5.66000000e-02,\n         2.22120000e-01,   5.20140000e-01,   1.38100000e-02,\n         8.26725000e+00,   6.88800000e-02,   4.30100000e-02,\n         1.36000000e-02,   3.32105000e+00,   5.73500000e-02,\n         3.25430000e-01,   3.83684000e+00,   3.73800000e-02,\n         1.68118000e+01,   3.11300000e-02,   1.13290000e-01,\n         1.28023000e+01,   2.44668000e+00,   4.75470000e-01,\n         6.79208000e+01,   8.26500000e-02,   1.73310000e-01,\n         1.78667000e+01,   5.30200000e-02,   1.70040000e-01,\n         1.49320000e-01,   1.00840000e-01,   3.77498000e+00,\n         2.79570000e-01,   1.91860000e-01,   8.19900000e-02,\n         3.23700000e-02,   1.53800000e-02,   1.55757000e+01,\n         3.51000000e-02,   2.43938000e+01,   7.75223000e+00,\n         1.78990000e-01,   5.64600000e-02,   1.39140000e-01,\n         4.66600000e-02,   5.36000000e-02,   1.14250000e-01,\n         8.02710000e-01,   5.40110000e-01,   9.91655000e+00,\n         1.49632000e+00,   1.80846000e+01,   1.08342000e+01,\n         1.88360000e-01,   5.90050000e-01,   2.41030000e-01,\n         1.17470000e-01,   1.44383000e+01,   4.41780000e-01,\n         9.76170000e-01,   2.11240000e-01,   1.39600000e-01,\n         6.21100000e-02,   1.11081000e+01,   2.29270000e-01,\n         1.44208000e+01,   6.65492000e+00,   1.18123000e+01,\n         7.02200000e-02,   1.96091000e+01,   1.25179000e+00,\n         1.07930000e-01,   8.44700000e-02,   3.15330000e-01,\n         2.36482000e+01,   2.53560000e-01,   2.05500000e-02,\n         2.89900000e-02,   6.91100000e-02,   2.37934000e+00,\n         4.41700000e-02,   1.41030000e-01,   3.04900000e-02,\n         3.93200000e-02,   9.32909000e+00,   9.96654000e+00,\n         1.30751000e+01,   2.89550000e-01,   1.91330000e-01,\n         4.75237000e+00,   2.92400000e+00,   2.63630000e-01,\n         2.17190000e-01,   9.39063000e+00,   2.17700000e-02,\n         9.59571000e+00,   2.14090000e-01,   4.74100000e-02,\n         6.58800000e-02,   6.27390000e-01,   1.19511000e+01,\n         6.80117000e+00,   9.26600000e-02,   8.18700000e-02,\n         3.61500000e-02]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-403-6214a1be53f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mscore_func_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36mchi2\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input X must be non-negative.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Aditya Kharosekar\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([  3.29820000e-01,   1.96570000e-01,   8.64476000e+00,\n         2.90900000e-01,   7.50300000e-02,   4.12380000e-01,\n         1.10874000e+01,   1.43337000e+01,   2.29690000e-01,\n         5.05900000e-02,   6.11540000e-01,   1.84982000e+01,\n         8.30800000e-02,   6.86000000e-02,   9.55770000e-01,\n         2.28760000e-01,   8.24809000e+00,   8.52040000e-01,\n         2.59150000e-01,   6.41700000e-02,   1.02330000e+01,\n         5.47900000e-02,   1.10270000e-01,   3.76800000e-02,\n         8.71675000e+00,   2.98190000e-01,   2.63548000e+00,\n         5.44520000e-01,   5.87205000e+00,   8.15174000e+00,\n         4.26131000e+00,   4.59000000e-02,   8.38700000e-02,\n         4.64689000e+00,   5.40500000e-01,   2.61690000e-01,\n         8.05579000e+00,   1.71340000e-01,   5.69175000e+00,\n         1.19294000e+00,   8.89762000e+01,   1.35540000e-01,\n         2.07460000e-01,   3.53700000e-02,   7.02259000e+00,\n         5.66637000e+00,   1.35870000e-01,   6.12900000e-02,\n         1.90730000e-01,   1.61282000e+00,   4.81900000e-02,\n         1.00080000e-01,   2.54300000e-02,   4.81213000e+00,\n         4.29700000e-02,   4.55587000e+00,   2.10380000e-01,\n         1.34284000e+00,   2.72900000e-02,   1.43900000e-02,\n         2.44953000e+00,   3.56868000e+00,   1.30100000e-02,\n         2.36862000e+00,   3.57800000e-02,   1.29320000e-01,\n         2.98500000e-02,   2.06080000e-01,   4.34879000e+00,\n         6.26300000e-02,   5.34120000e-01,   4.29400000e-02,\n         3.76619000e+01,   1.00623000e+01,   3.87100000e-02,\n         3.04100000e-02,   6.72400000e-02,   9.82349000e+00,\n         1.20742000e+00,   6.15100000e-02,   1.54450000e-01,\n         8.98296000e+00,   4.11300000e-02,   1.06120000e-01,\n         6.37960000e-01,   5.60200000e-02,   1.67600000e-01,\n         3.69695000e+00,   7.89600000e-02,   2.39120000e-01,\n         1.62864000e+00,   3.84970000e+00,   2.81838000e+00,\n         4.37900000e-02,   4.89822000e+00,   1.64390000e-01,\n         1.27570000e-01,   5.20580000e-01,   1.80028000e+00,\n         1.39134000e+01,   2.31390000e+00,   5.73116000e+00,\n         4.87141000e+00,   5.29305000e+00,   2.50461000e+01,\n         9.18702000e+00,   5.37200000e-02,   2.21880000e-01,\n         4.22239000e+00,   6.32000000e-03,   4.01100000e-02,\n         8.37000000e-02,   9.88430000e-01,   9.92485000e+00,\n         4.68400000e-02,   1.30580000e-01,   3.51140000e-01,\n         1.36781000e+01,   9.84900000e-02,   8.87300000e-02,\n         9.72418000e+00,   1.62110000e-01,   7.25800000e-01,\n         6.71772000e+00,   8.01400000e-02,   6.28807000e+00,\n         8.81250000e-01,   4.92980000e-01,   7.95000000e-02,\n         1.40520000e-01,   9.06000000e-03,   7.72990000e-01,\n         3.58090000e-01,   1.41500000e-01,   3.52330000e-01,\n         3.39830000e-01,   6.89900000e-02,   1.04690000e-01,\n         1.70900000e-02,   6.71910000e-01,   3.15000000e-02,\n         7.16500000e-02,   1.25790000e-01,   4.54192000e+00,\n         3.30450000e-01,   3.65900000e-02,   3.03470000e-01,\n         9.60400000e-02,   2.83920000e-01,   3.54800000e-02,\n         4.07710000e-01,   9.37800000e-02,   1.10690000e-01,\n         2.00900000e-02,   6.04700000e-02,   1.88110000e+01,\n         5.02300000e-02,   1.22472000e+01,   1.06718000e+01,\n         8.24400000e-02,   1.43200000e-02,   1.32620000e-01,\n         6.23560000e-01,   9.51200000e-02,   9.23230000e+00,\n         2.33099000e+00,   6.90500000e-02,   7.83932000e+00,\n         1.65660000e+00,   4.56000000e-02,   3.22640000e-01,\n         8.70700000e-02,   7.01300000e-02,   1.38799000e+00,\n         7.88600000e-02,   2.87500000e-02,   1.00245000e+00,\n         4.52700000e-02,   3.70500000e-02,   2.45220000e-01,\n         2.15505000e+00,   1.13081000e+00,   1.15172000e+00,\n         7.97800000e-02,   2.48017000e+01,   1.59360000e-01,\n         5.82115000e+00,   9.16400000e-02,   8.49213000e+00,\n         3.87350000e-01,   5.78000000e-02,   3.46600000e-02,\n         4.09740000e+00,   1.87000000e-02,   4.15292000e+01,\n         1.23290000e-01,   5.50070000e-01,   2.49800000e-01,\n         5.20177000e+00,   9.10300000e-02,   6.64200000e-02,\n         9.25200000e-02,   7.84200000e-01,   3.67822000e+00,\n         6.39312000e+00,   3.40060000e-01,   7.40389000e+00,\n         1.46336000e+00,   2.89600000e-01,   1.81590000e-01,\n         7.36711000e+00,   1.58760000e-01,   1.42362000e+01,\n         6.12700000e-02,   8.82900000e-02,   9.51363000e+00,\n         5.11830000e-01,   5.18800000e-02,   2.37857000e+00,\n         6.29760000e-01,   2.53870000e-01,   3.69311000e+00,\n         3.49400000e-01,   1.42310000e-01,   3.18270000e-01,\n         3.53501000e+00,   3.41090000e-01,   1.83377000e+00,\n         7.05042000e+00,   6.96215000e+00,   2.99160000e-01,\n         7.87500000e-02,   2.20511000e+01,   1.83370000e-01,\n         5.42500000e-02,   9.29900000e-02,   6.56650000e-01,\n         1.27440000e-01,   5.66998000e+00,   1.22690000e-01,\n         1.77800000e-02,   7.35341000e+01,   7.15100000e-02,\n         3.50200000e-02,   9.06500000e-02,   4.42228000e+00,\n         5.37000000e-01,   1.33598000e+01,   4.03841000e+00,\n         3.47428000e+00,   1.50100000e-02,   1.44550000e-01,\n         8.22100000e-02,   5.82401000e+00,   1.02900000e-01,\n         3.44500000e-02,   1.31170000e-01,   2.86558000e+01,\n         4.33700000e-02,   7.52601000e+00,   1.74460000e-01,\n         2.24236000e+00,   5.26930000e-01,   2.14918000e+00,\n         9.17800000e-02,   2.18700000e-02,   5.78900000e-02,\n         2.51990000e-01,   6.44405000e+00,   2.25971000e+01,\n         6.63510000e-01,   8.40540000e-01,   4.66883000e+00,\n         1.23247000e+00,   2.24890000e-01,   2.68380000e-01,\n         7.85700000e-01,   1.41385000e+00,   5.66000000e-02,\n         2.22120000e-01,   5.20140000e-01,   1.38100000e-02,\n         8.26725000e+00,   6.88800000e-02,   4.30100000e-02,\n         1.36000000e-02,   3.32105000e+00,   5.73500000e-02,\n         3.25430000e-01,   3.83684000e+00,   3.73800000e-02,\n         1.68118000e+01,   3.11300000e-02,   1.13290000e-01,\n         1.28023000e+01,   2.44668000e+00,   4.75470000e-01,\n         6.79208000e+01,   8.26500000e-02,   1.73310000e-01,\n         1.78667000e+01,   5.30200000e-02,   1.70040000e-01,\n         1.49320000e-01,   1.00840000e-01,   3.77498000e+00,\n         2.79570000e-01,   1.91860000e-01,   8.19900000e-02,\n         3.23700000e-02,   1.53800000e-02,   1.55757000e+01,\n         3.51000000e-02,   2.43938000e+01,   7.75223000e+00,\n         1.78990000e-01,   5.64600000e-02,   1.39140000e-01,\n         4.66600000e-02,   5.36000000e-02,   1.14250000e-01,\n         8.02710000e-01,   5.40110000e-01,   9.91655000e+00,\n         1.49632000e+00,   1.80846000e+01,   1.08342000e+01,\n         1.88360000e-01,   5.90050000e-01,   2.41030000e-01,\n         1.17470000e-01,   1.44383000e+01,   4.41780000e-01,\n         9.76170000e-01,   2.11240000e-01,   1.39600000e-01,\n         6.21100000e-02,   1.11081000e+01,   2.29270000e-01,\n         1.44208000e+01,   6.65492000e+00,   1.18123000e+01,\n         7.02200000e-02,   1.96091000e+01,   1.25179000e+00,\n         1.07930000e-01,   8.44700000e-02,   3.15330000e-01,\n         2.36482000e+01,   2.53560000e-01,   2.05500000e-02,\n         2.89900000e-02,   6.91100000e-02,   2.37934000e+00,\n         4.41700000e-02,   1.41030000e-01,   3.04900000e-02,\n         3.93200000e-02,   9.32909000e+00,   9.96654000e+00,\n         1.30751000e+01,   2.89550000e-01,   1.91330000e-01,\n         4.75237000e+00,   2.92400000e+00,   2.63630000e-01,\n         2.17190000e-01,   9.39063000e+00,   2.17700000e-02,\n         9.59571000e+00,   2.14090000e-01,   4.74100000e-02,\n         6.58800000e-02,   6.27390000e-01,   1.19511000e+01,\n         6.80117000e+00,   9.26600000e-02,   8.18700000e-02,\n         3.61500000e-02]),)"
     ]
    }
   ],
   "source": [
    "X_train_new = SelectKBest(chi2, k=2).fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
